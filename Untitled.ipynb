{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSPy로 LLM을 프롬프팅이 아니라 프로그래밍하기 (Programming, not Prompting)\n",
    "  \n",
    "Ref : [Source](https://github.com/ALucek/dspy-breakdown/blob/main/dspy_breakdown.ipynb), [YouTube](https://youtu.be/Zv4LjO8teqE?si=3XSPg21FOzqN3tdH)\n",
    "  \n",
    "**[DSPy(Declarative Self-improving Python)](https://dspy.ai/)** 는 스탠퍼드 NLP에서 개발한 프레임워크로, 언어 모델을 프롬프트 템플릿이 아니라 프로그래밍 가능한 함수로 다룬다. DSPy는 PyTorch와 유사한 인터페이스를 제공하여 LLM 연산을 정의하고, 조합하며, 최적화할 수 있게 한다.\n",
    "\n",
    "개발자는 복잡한 프롬프트를 직접 작성하고 유지하는 대신, **입·출력 시그니처만 선언**하면 되고, **프롬프트 엔지니어링과 최적화는 DSPy가 자동으로 처리**한다. 이 프레임워크는 자동 프롬프트 튜닝과 자기 개선(self-improvement)과 같은 기법을 통해 LLM 파이프라인을 체계적으로 개선할 수 있도록 한다.\n",
    "\n",
    "<img src=\"./Media/dspy_workflow.png\">\n",
    "\n",
    "DSPy의 워크플로우는 다음의 네 가지 핵심 단계로 구성된다:\n",
    "\n",
    "* 시그니처(Signature) 와 모듈(Module)을 사용해 프로그램을 정의한다.\n",
    "* 프로그램의 성능을 명확하게 보여줄 수 있는 측정 가능한 성공 지표를 설계한다.\n",
    "* 프로그램을 컴파일하고, 설정한 성공 지표를 기준으로 최적화한다.\n",
    "* 추가 데이터를 수집하고, 이를 바탕으로 반복적으로 개선한다.\n",
    "\n",
    "이 노트북에서는 이러한 단계 전반에 걸쳐 DSPy가 제공하는 다양한 접근 방식들을 살펴보고 실제로 적용해본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "<img src=\"./Media/dspy.png\" width=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure LLM\n",
    "\n",
    "DSPy는 기본적으로 환경 전반에 걸쳐 모델과 응답을 캐시한다. 별도로 명시하지 않는 한, 한 번 언어 모델을 설정하면 이후의 모든 호출에서는 해당 언어 모델이 자동으로 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM('openai/gpt-4o-mini')\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a test! How can I assist you further?']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm(messages=[{\"role\":\"user\", \"content\":\"Say this is a test!\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 시그니처(Signatures)\n",
    "\n",
    "DSPy의 시그니처는 일반적인 함수 시그니처와 동일한 개념을 따르지만, 자연어로 정의된다는 점이 특징이다. 이는 DSPy가 대체하고자 하는 기존의 “프롬프팅(prompting)” 방식의 핵심에 해당한다. LLM에게 무엇을 하라고 지시하는 대신, LLM이 무엇을 하게 될지를 선언적으로 정의하는 접근을 취한다.\n",
    "\n",
    "기본 형식은 다음과 같다.\n",
    "\n",
    "'input -> output'\n",
    "\n",
    "입력과 출력은 원하는 어떤 형태든 정의할 수 있으며, 여러 개의 입력과 출력, 타입 정보, 혹은 더 명확하게 구조화된 스키마를 함께 선언하는 것도 가능하다.\n",
    "\n",
    "<img src=\"./Media/signatures.png\">\n",
    "\n",
    "내부적으로는 여전히 언어 모델을 위한 프롬프트가 사용되지만, 자연어로 정의한 시그니처를 기반으로 고정된 프롬프트가 아니라 모듈화된 형태로 동작한다. 즉, 시그니처에 따라 표현 방식과 구조가 동적으로 바뀌도록 설계되어 있다.\n",
    "\n",
    "프롬프팅을 추상화한다는 점에서 다소 직관에 어긋나게 느껴질 수 있지만, DSPy는 이러한 구조를 통해 모델을 쉽게 교체할 수 있고, 이후에 살펴볼 알고리즘 수준의 최적화도 가능하도록 설계되었다.\n",
    "\n",
    "### 단순한 입력과 출력(Simple Input & Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  The sky appears blue due to a phenomenon known as Rayleigh scattering. When sunlight enters the Earth's atmosphere, it collides with molecules and small particles in the air. Sunlight is made up of many colors, each with different wavelengths. Blue light has a shorter wavelength and is scattered more than the other colors when it strikes these air particles. As a result, we see a blue sky during the day. In contrast, during sunrise and sunset, the sky can appear orange or red because the sunlight travels a longer distance through the atmosphere, scattering the shorter blue wavelengths and allowing the longer red wavelengths to become more prominent.\n"
     ]
    }
   ],
   "source": [
    "qna = dspy.Predict('question -> answer')\n",
    "response = qna(question=\"Why is the sky blue\")\n",
    "print(\"Response: \", response.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2026-01-19T15:57:22.621547]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `question` (str):\n",
      "Your output fields are:\n",
      "1. `answer` (str):\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## answer ## ]]\n",
      "{answer}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Given the fields `question`, produce the fields `answer`.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## question ## ]]\n",
      "Why is the sky blue\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## answer ## ]]\n",
      "The sky appears blue due to a phenomenon known as Rayleigh scattering. When sunlight enters the Earth's atmosphere, it collides with molecules and small particles in the air. Sunlight is made up of many colors, each with different wavelengths. Blue light has a shorter wavelength and is scattered more than the other colors when it strikes these air particles. As a result, we see a blue sky during the day. In contrast, during sunrise and sunset, the sky can appear orange or red because the sunlight travels a longer distance through the atmosphere, scattering the shorter blue wavelengths and allowing the longer red wavelengths to become more prominent.\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lm.inspect_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:  The document outlines the competitive landscape for the company's products, highlighting the intense competition driven by rapid technological change and varying industry standards. Key competitive factors include product performance, range of offerings, customer access, and manufacturing capabilities. The company anticipates increased competition from existing and new competitors, potentially with lower prices or enhanced product features. Competitors in the GPU, CPU, DPU, and AI computing sectors pose a significant threat, particularly those with superior resources. The company recognizes the necessity to anticipate market demands and maintain quality and pricing to remain competitive.\n"
     ]
    }
   ],
   "source": [
    "summ = dspy.Predict('document -> summary')\n",
    "\n",
    "document = \"\"\"\n",
    "The market for our products is intensely competitive and is characterized by rapid technological change and evolving industry standards. \n",
    "We believe that theprincipal competitive factors in this market are performance, breadth of product offerings, access to customers and partners and distribution channels, softwaresupport, conformity to industry standard APIs, manufacturing capabilities, processor pricing, and total system costs. \n",
    "We believe that our ability to remain competitive will depend on how well we are able to anticipate the features and functions that customers and partners will demand and whether we are able todeliver consistent volumes of our products at acceptable levels of quality and at competitive prices. \n",
    "We expect competition to increase from both existing competitors and new market entrants with products that may be lower priced than ours or may provide better performance or additional features not provided by our products. \n",
    "In addition, it is possible that new competitors or alliances among competitors could emerge and acquire significant market share.\n",
    "A significant source of competition comes from companies that provide or intend to provide GPUs, CPUs, DPUs, embedded SoCs, and other accelerated, AI computing processor products, and providers of semiconductor-based high-performance interconnect products based on InfiniBand, Ethernet, Fibre Channel,and proprietary technologies. \n",
    "Some of our competitors may have greater marketing, financial, distribution and manufacturing resources than we do and may bemore able to adapt to customers or technological changes. \n",
    "We expect an increasingly competitive environment in the future.\n",
    "\"\"\"\n",
    "\n",
    "response = summ(document=document)\n",
    "print(\"Summary: \", response.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Inputs and Outputs\n",
    "\n",
    "<img src='./Media/multiple_signature.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  Your name is Adam Lucek.\n",
      "\n",
      "Citation:  Context provided indicates that you are Adam Lucek.\n"
     ]
    }
   ],
   "source": [
    "multi = dspy.Predict('question, context -> answer, citation')\n",
    "\n",
    "question = \"What's my name?\"\n",
    "context = \"The user you're talking to is Adam Lucek, AI youtuber extraordinare\"\n",
    "\n",
    "response = multi(question=question, context=context)\n",
    "\n",
    "print(\"Answer: \", response.answer)\n",
    "print(\"\\nCitation: \", response.citation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type Hints with Outputs\n",
    "\n",
    "<img src=\"./Media/input_type.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Classification:  negative\n",
      "\n",
      "Confidence:  0.85\n",
      "\n",
      "Reasoning:  The phrase \"I didn't really like it\" indicates a lack of positive feelings towards whatever is being referred to, suggesting a negative sentiment. The use of \"don't quite know\" adds uncertainty, but the overall expression clearly leans towards disfavor.\n"
     ]
    }
   ],
   "source": [
    "emotion = dspy.Predict('input -> sentiment: str, confidence: float, reasoning: str')\n",
    "\n",
    "text = \"I don't quite know, I didn't really like it\"\n",
    "\n",
    "response = emotion(input=text)\n",
    "\n",
    "print(\"Sentiment Classification: \", response.sentiment)\n",
    "print(\"\\nConfidence: \", response.confidence)\n",
    "print(\"\\nReasoning: \", response.reasoning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 클래스 기반 시그니처(Class-Based Signatures)\n",
    "\n",
    "보다 고급 시그니처를 위해, DSPy는 단순한 인라인 문자열 방식 대신 Pydantic 클래스 또는 데이터 구조 스키마를 정의할 수 있도록 지원한다. 이러한 클래스는 기본적으로 dspy.Signature를 상속받아야 하며, 입력 필드는 dspy.InputField(), 출력 필드는 dspy.OutputField()를 사용해 명시적으로 정의해야 한다.\n",
    "\n",
    "각 필드에는 선택적으로 desc 인자를 전달할 수 있으며, 이를 통해 해당 필드에 대한 추가적인 맥락이나 설명을 함께 제공할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Text:  In a quaint coffee shop where dreams are brewed,  \n",
      "The finest lattes dance, their warmth imbued.  \n",
      "With each cup, a symphony of flavors arise,  \n",
      "Crafted by a barista, a master in disguise.  \n",
      "His hands weave magic with the espresso's grace,  \n",
      "Creating art in a cup that time can't erase.\n",
      "\n",
      "Style Metrics:  {'formality': 0.7, 'complexity': 0.6, 'emotiveness': 0.8}\n",
      "\n",
      "Preserver Keywords:  ['coffee shop', 'best', 'lattes', 'barista', 'espresso', 'machine']\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "\n",
    "class TextStyleTransfer(dspy.Signature):\n",
    "    \"\"\"Transfer text between different writing styles while preserving content.\"\"\"\n",
    "    text: str = dspy.InputField()\n",
    "    source_style: Literal[\"academic\", \"casual\", \"business\", \"poetic\"] = dspy.InputField()\n",
    "    target_style: Literal[\"academic\", \"casual\", \"business\", \"poetic\"] = dspy.InputField()\n",
    "    preserved_keywords: list[str] = dspy.OutputField()\n",
    "    transformed_text: str = dspy.OutputField()\n",
    "    style_metrics: dict[str, float] = dspy.OutputField(desc=\"Scores for formality, complexity, emotiveness\")\n",
    "\n",
    "text = \"This coffee shop makes the best lattes ever! Their new barista really knows what he's doing with the espresso machine.\"\n",
    "\n",
    "style_transfer = dspy.Predict(TextStyleTransfer)\n",
    "\n",
    "response = style_transfer(\n",
    "    text=text,\n",
    "    source_style=\"casual\",\n",
    "    target_style=\"poetic\"\n",
    ")\n",
    "\n",
    "print(\"Transformed Text: \", response.transformed_text)\n",
    "print(\"\\nStyle Metrics: \", response.style_metrics)\n",
    "print(\"\\nPreserver Keywords: \", response.preserved_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모듈(Modules)\n",
    "\n",
    "<img src=\"./Media/modules.png\">\n",
    "\n",
    "모듈은 시그니처에 다양한 프롬프팅 전략을 적용하는 계층이다. 앞선 시그니처 예제에서는 기본적인 Predict 모듈을 사용했지만, DSPy에는 이 외에도 널리 사용되는 여러 전략과 그 변형들이 준비되어 있다. 다음은 현재 제공되는 주요 모듈들이다.\n",
    "\n",
    "* **ChainOfThought**: 출력을 생성하기 전에 추론 단계를 먼저 유도하는 체인-오브-쏘트(chain-of-thought) 프롬프팅을 구현한다. 이 모듈은 모델이 구조화된 사고를 하도록 \"Let's think step by step\"과 같은 문구를 자동으로 추가한다. 복잡한 문제를 여러 단계로 나누어 사고해야 하는 경우에 적합하다.\n",
    "  \n",
    "* **ProgramOfThought**: 문제 해결을 위해 실행 가능한 Python 코드를 생성하며, 오류 처리와 코드 재생성 기능을 기본적으로 포함한다. 수학적 문제나 알고리즘적 문제처럼 실제 코드 실행을 통해 해결하는 것이 더 효과적인 경우에 사용한다.\n",
    "\n",
    "* **ReAct**: 사고(Reasoning), 행동(Acting: 도구 사용), 관찰(Observation)을 구조화된 루프 형태로 번갈아 수행하는 ReAct 방식을 구현한다.여러 단계의 추론과 외부 도구나 API와의 상호작용이 필요한 작업에 적합하다.\n",
    "\n",
    "그리고 몇 가지 헬퍼 모듈:\n",
    "\n",
    "* **MultiChainComparison**: 여러 번의 추론 시도(기본값 3회)를 수행한 뒤, 서로 다른 추론 경로를 비교하여 더 정확한 하나의 응답으로 통합한다. 문제 해결의 정확도가 중요하고, 여러 번의 시도를 감당할 수 있는 경우에 적합하다.\n",
    "\n",
    "* **Majority**: 여러 개의 응답(completion)을 입력으로 받아 텍스트를 정규화한 뒤, 가장 많이 등장한 응답을 반환하는 유틸리티 함수다. 여러 번의 생성 결과에 대해 간단한 투표 방식을 적용해 신뢰도를 높이고 싶을 때 유용하다.\n",
    "\n",
    "### Chain of Thought\n",
    "\n",
    "<img src=\"./Media/cot_module.png\" width=\"300\">\n",
    "\n",
    "**ChainOfThought**는 출력 전에 명시적인 추론 단계를 포함하도록 프롬프트 시그니처를 수정하는 방식으로 동작한다. 시그니처로 초기화되면, \"Reasoning: Let's think step by step in order to\"라는 접두어를 가진 reasoning 필드를 앞에 추가한 확장 시그니처를 생성한다. 이 reasoning 필드는 언어 모델이 최종 답변을 제공하기에 앞서 사고 과정을 먼저 작성하도록 강제한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment:  mixed\n",
      "\n",
      "Reasoning:  The input expresses a mixed emotion towards an experience. The term \"phenomenal\" indicates a strong positive reaction, suggesting that the experience was impressive or of high quality. However, the phrase \"but I hated it!\" introduces a conflicting negative sentiment, implying that despite recognizing the quality, there is a deep-seated dislike or discomfort associated with it. This juxtaposition reveals complex feelings that do not align neatly into a single sentiment category.\n"
     ]
    }
   ],
   "source": [
    "# Define the Signature and Module\n",
    "cot_emotion = dspy.ChainOfThought('input -> sentiment: str')\n",
    "\n",
    "# Example\n",
    "text = \"That was phenomenal, but I hated it!\"\n",
    "\n",
    "# Run\n",
    "cot_response = cot_emotion(input=text)\n",
    "\n",
    "# Output\n",
    "print(\"Sentiment: \", cot_response.sentiment)\n",
    "# Inherently added reasoning\n",
    "print(\"\\nReasoning: \", cot_response.reasoning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2026-01-19T15:57:28.394008]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `input` (str):\n",
      "Your output fields are:\n",
      "1. `reasoning` (str): \n",
      "2. `sentiment` (str):\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## input ## ]]\n",
      "{input}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## sentiment ## ]]\n",
      "{sentiment}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Given the fields `input`, produce the fields `sentiment`.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## input ## ]]\n",
      "That was phenomenal, but I hated it!\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## sentiment ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## reasoning ## ]]\n",
      "The input expresses a mixed emotion towards an experience. The term \"phenomenal\" indicates a strong positive reaction, suggesting that the experience was impressive or of high quality. However, the phrase \"but I hated it!\" introduces a conflicting negative sentiment, implying that despite recognizing the quality, there is a deep-seated dislike or discomfort associated with it. This juxtaposition reveals complex feelings that do not align neatly into a single sentiment category.\n",
      "\n",
      "[[ ## sentiment ## ]]\n",
      "mixed\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lm.inspect_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program of Thought\n",
    "\n",
    "<img src=\"./Media/program_of_thought.png\">\n",
    "\n",
    "ProgramOfThought(PoT)는 자연어 출력에 직접 의존하는 대신, 실행 가능한 Python 코드를 생성하여 과제를 해결한다. 작업이 주어지면, PoT는 먼저 ChainOfThought 예측기를 사용해 Python 코드를 생성한 뒤, 이를 격리된 Python 인터프리터 환경에서 실행한다. 코드 실행 중 오류가 발생하면, PoT는 해당 오류를 언어 모델에 다시 전달하고 수정된 코드를 생성하도록 요청하는 개선 루프에 들어간다. 이 과정은 최대 지정된 반복 횟수(기본값 3회)까지 반복된다. 최종 출력은 언어 모델이 직접 생성한 텍스트가 아니라, 성공적으로 실행된 코드의 실제 실행 결과에서 얻어진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Signature\n",
    "class MathAnalysis(dspy.Signature):\n",
    "    \"\"\"Analyze a dataset and compute various statistical metrics.\"\"\"\n",
    "    numbers: list[float] = dspy.InputField(desc=\"List of numerical values to analyze\")\n",
    "    required_metrics: list[str] = dspy.InputField(desc=\"List of metrics to calculate (e.g. ['mean', variance', 'quartiles'])\")\n",
    "    analysis_results: dict[str, float] = dspy.OutputField(desc=\"Dictionary containing the calculated metrics\")\n",
    "\n",
    "# Create the module\n",
    "math_analyzer = dspy.ProgramOfThought(MathAnalysis)\n",
    "\n",
    "# Example\n",
    "data = [1.5, 2.8, 3.2, 4.7, 5.1, 2.3, 3.9]\n",
    "metrics = ['mean', 'median']\n",
    "\n",
    "# Run\n",
    "pot_response = math_analyzer(\n",
    "    numbers=data,\n",
    "    required_metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning:  In the provided code, we are using the NumPy library to calculate the mean and median of the list of numbers. The mean is calculated as the average of the numbers, while the median is the middle value when the numbers are sorted. The expected output should be a dictionary containing these two metrics. Since the output doesn't provide specific values, I will compute the mean and median for the supplied numbers to prepare the final results.\n",
      "\n",
      "Results:  {'mean': 3.4285714285714284, 'median': 3.2}\n"
     ]
    }
   ],
   "source": [
    "print(\"Reasoning: \", pot_response.reasoning)\n",
    "print(\"\\nResults: \", pot_response.analysis_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2026-01-19T15:57:32.274117]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `numbers` (list[float]): List of numerical values to analyze\n",
      "2. `required_metrics` (list[str]): List of metrics to calculate (e.g. ['mean', variance', 'quartiles'])\n",
      "3. `final_generated_code` (str): python code that answers the question\n",
      "4. `code_output` (str): output of previously-generated python code\n",
      "Your output fields are:\n",
      "1. `reasoning` (str): \n",
      "2. `analysis_results` (dict[str, float]): Dictionary containing the calculated metrics\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## numbers ## ]]\n",
      "{numbers}\n",
      "\n",
      "[[ ## required_metrics ## ]]\n",
      "{required_metrics}\n",
      "\n",
      "[[ ## final_generated_code ## ]]\n",
      "{final_generated_code}\n",
      "\n",
      "[[ ## code_output ## ]]\n",
      "{code_output}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## analysis_results ## ]]\n",
      "{analysis_results}        # note: the value you produce must adhere to the JSON schema: {\"type\": \"object\", \"additionalProperties\": {\"type\": \"number\"}}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Given the final code `numbers`, `required_metrics`, `final_generated_code`, `code_output`, provide the final `analysis_results`.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## numbers ## ]]\n",
      "[1.5, 2.8, 3.2, 4.7, 5.1, 2.3, 3.9]\n",
      "\n",
      "[[ ## required_metrics ## ]]\n",
      "[\"mean\", \"median\"]\n",
      "\n",
      "[[ ## final_generated_code ## ]]\n",
      "import numpy as np\n",
      "\n",
      "numbers = [1.5, 2.8, 3.2, 4.7, 5.1, 2.3, 3.9]\n",
      "mean = np.mean(numbers)\n",
      "median = np.median(numbers)\n",
      "\n",
      "analysis_results = {\n",
      "    \"mean\": mean,\n",
      "    \"median\": median\n",
      "}\n",
      "\n",
      "analysis_results\n",
      "\n",
      "[[ ## code_output ## ]]\n",
      "\"Loading numpy-tests\"\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## analysis_results ## ]]` (must be formatted as a valid Python dict[str, float]), and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## reasoning ## ]]\n",
      "In the provided code, we are using the NumPy library to calculate the mean and median of the list of numbers. The mean is calculated as the average of the numbers, while the median is the middle value when the numbers are sorted. The expected output should be a dictionary containing these two metrics. Since the output doesn't provide specific values, I will compute the mean and median for the supplied numbers to prepare the final results.\n",
      "\n",
      "[[ ## analysis_results ## ]]\n",
      "{\"mean\": 3.4285714285714284, \"median\": 3.2}\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lm.inspect_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasoning + Acting (ReAct)\n",
    "\n",
    "<img src=\"./Media/react.png\">\n",
    "\n",
    "ReAct는 **추론(reasoning)** 과 **도구 사용(tool usage)** 을 결합하여 상호작용적인 문제 해결을 가능하게 한다. 이는 모델이 **생각–행동(thought–action) 쌍의 흐름(trajectory)** 을 유지하면서 작동하는 방식으로, 각 단계마다 모델은 자신의 추론을 설명하고, 사용할 도구를 선택하며, 해당 도구에 전달할 인자를 제공한 뒤, 도구 실행 결과를 관찰하여 다음 단계를 결정한다. 각 반복(iteration)은 네 가지 요소로 구성된다. 즉, 전략을 설명하는 **생각(thought)**, 사용 가능한 도구 중 하나를 선택하는 **도구 선택**, 도구에 전달할 **인자(arguments)**, 그리고 도구를 실행한 결과인 **관찰(observation)** 이다. 이 과정은 모델이 스스로 “finish”를 선택하거나 최대 반복 횟수에 도달할 때까지 계속된다. 아래는 간단한 예시이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  The Baltimore Orioles won the World Series in 1983, and England won the World Cup in 1966.\n",
      "\n",
      "Reasoning:  The 1983 World Series was won by the Baltimore Orioles, who defeated the Philadelphia Phillies. In the 1966 World Cup, England emerged victorious, winning their first and only World Cup by beating West Germany in the final.\n"
     ]
    }
   ],
   "source": [
    "# Define a Tool\n",
    "def wikipedia_search(query: str) -> list[str]:\n",
    "    \"\"\"Retrieves abstracts from Wikipedia.\"\"\"\n",
    "    # Existing Wikipedia Abstracts Server\n",
    "    results = dspy.ConBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')(query, k=3)\n",
    "    return [x['text'] for x in results]\n",
    "\n",
    "# Define ReAct Module\n",
    "react_module = dspy.ReAct('question -> response', tools=[wikipedia_search])\n",
    "\n",
    "# Example\n",
    "text = \"Who won the world series in 1983 and who won the world cup in 1966?\"\n",
    "\n",
    "# Run\n",
    "react_response = react_module(question=text)\n",
    "\n",
    "print(\"Answer: \", react_response.response)\n",
    "print(\"\\nReasoning: \", react_response.reasoning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Chain Comparison\n",
    "\n",
    "<img src=\"./Media/multi_chain.png\">\n",
    "\n",
    "MultiChainComparison은 여러 개의 기존 응답(completion)을 하나의 더 견고한 최종 예측으로 종합하는 **메타 예측기(meta-predictor)** 이다. 이 모듈은 스스로 예측을 생성하지 않고, 대신 다른 예측기들로부터 **M개의 서로 다른 응답(기본값 3)** 을 입력으로 받는다. 이 응답들은 동일한 예측기를 서로 다른 temperature로 실행한 결과일 수도 있고, 완전히 다른 예측기에서 나온 결과일 수도 있으며, 혹은 동일한 설정으로 여러 번 호출한 결과일 수도 있다.\n",
    "\n",
    "각 응답은 Student Attempt #1:, Student Attempt #2: 와 같은 형식으로 정리되며,\n",
    "각 시도는 «나는 [추론 과정]을 바탕으로 생각해 보았고, 확신은 없지만 내 예측은 [답]이다»\n",
    "와 같은 구조로 패키징된다.\n",
    "이후 모듈은 “Accurate Reasoning: Thank you everyone. Let’s now holistically…”와 같은 프롬프트를 사용해, 모델이 이 여러 시도를 종합적으로 분석·비교·비판하도록 유도하고, 그 결과로 하나의 최종 답변을 도출한다.\n",
    "\n",
    "이 접근법은 모델이 최종 결정을 내리기 전에 여러 해결 경로를 명시적으로 비교하고 검토하게 함으로써, 개별 예측에서 발생할 수 있는 오류를 완화하는 데 도움을 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Positive\n",
      "\n",
      "Reasoning: The use of \"piculiar\" (presumably a typo for \"peculiar\") conveys a sense of something unusual or strange, which can evoke feelings of surprise or curiosity. The exclamation mark suggests a heightened emotional response, indicating that the situation was noteworthy. Overall, the sentiment leans towards surprise or bemusement rather than negativity, leading to the conclusion that the sentiment is likely positive or curious.\n",
      "\n",
      "Completion 1:  Prediction(\n",
      "    reasoning='The use of the word \"piculiar\" suggests a sense of surprise or oddness about a situation. This can indicate a feeling of curiosity or confusion. The exclamation mark emphasizes the speaker\\'s strong reaction to whatever was experienced, suggesting that it was noteworthy or unexpected.',\n",
      "    sentiment='Neutral to mildly positive, as it expresses intrigue or interest without overt negativity.'\n",
      ")\n",
      "\n",
      "Completion 2:  Prediction(\n",
      "    reasoning='The word \"piculiar\" (likely a typo for \"peculiar\") suggests something strange or unusual. The tone indicates surprise or confusion, which may convey a sense of curiosity or bemusement about an unexpected situation.',\n",
      "    sentiment='Neutral'\n",
      ")\n",
      "\n",
      "Completion 3:  Prediction(\n",
      "    reasoning='The word \"piculiar\" seems to be a typographical error for \"peculiar.\" The sentiment expressed in the statement is one of confusion or surprise, as \"peculiar\" often refers to something strange or unusual. The use of the exclamation mark further emphasizes the speaker\\'s strong reaction to whatever they are describing as peculiar.',\n",
      "    sentiment='Surprised'\n",
      ")\n",
      "\n",
      "Completion 4:  Prediction(\n",
      "    reasoning='The word \"p peculiar\" suggests something unusual or strange, which can be interpreted as an observation that something was not quite right or unexpected. This creates a neutral to slightly negative connotation depending on the context of the peculiarness.',\n",
      "    sentiment='Neutral'\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Run CoT completions with increasing temperatures\n",
    "text = \"That was piculiar!\"   # strange, piculiar, exotic, phenominal\n",
    "\n",
    "cot_completions = []\n",
    "for i in range(4):\n",
    "    # Temperature increases: 0.7, 0.8, 0.9\n",
    "    temp_config = dict(temperature=0.7+(0.1*i))\n",
    "    completion = cot_emotion(input=text, config=temp_config)\n",
    "    cot_completions.append(completion)\n",
    "\n",
    "# Synthesize with MultiChainComparison\n",
    "mcot_emotion = dspy.MultiChainComparison('input -> sentiment', M=4)\n",
    "final_result = mcot_emotion(completions=cot_completions, input=text)\n",
    "\n",
    "print(f\"Sentiment: {final_result.sentiment}\")\n",
    "print(f\"\\nReasoning: {final_result.rationale}\")\n",
    "\n",
    "for i in range(4):\n",
    "    print(f\"\\nCompletion {i+1}: \", cot_completions[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Majority\n",
    "\n",
    "<img src=\"./Media/majority.png\">\n",
    "\n",
    "Majority는 여러 개의 응답(completion)을 대상으로 기본적인 투표 메커니즘을 적용해 가장 많이 등장한 답을 선택하는 유틸리티 함수다. 이 함수는 응답들을 포함한 Prediction 객체를 입력으로 받거나, 혹은 응답 리스트 자체를 직접 입력으로 받을 수 있다. 동작 과정에서 Majority는 대상 필드의 값을 정규화(normalization)하는데, 이때 사용할 필드는 명시적으로 지정할 수도 있고, 지정하지 않으면 기본적으로 마지막 출력 필드가 사용된다. 이 정규화 과정은 normalize_text 함수가 담당하며, 의미적으로 동일하지만 표현이 약간 다른 텍스트들을 같은 답으로 취급하도록 돕는다. 또한 무시되어야 할 답변의 경우 None을 반환하도록 처리한다. 표 수가 같은 경우(동률)에는 먼저 생성된 응답이 우선된다. 이 함수는 서로 다른 temperature로 예측기를 여러 번 실행하는 등, 다수의 응답을 생성하는 모듈들과 함께 사용할 때 특히 유용하며, 가장 일반적인 답을 간단히 선택할 수 있는 방법을 제공한다. 최종적으로 Majority는 선택된(승리한) 하나의 응답만을 포함하는 새로운 Prediction 객체를 반환한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common sentiment: Neutral\n"
     ]
    }
   ],
   "source": [
    "# Example Completions From Prior Multi-Chain\n",
    "majority_result = dspy.majority(cot_completions, field=\"sentiment\")\n",
    "\n",
    "# Results\n",
    "print(f\"Most common sentiment: {majority_result.sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluators\n",
    "\n",
    "모듈이 프로그램의 기본 구성 요소이긴 하지만, 프롬프트 체인을 반복 수정하듯이 모듈 자체를 직접 튜닝하거나 변경하는 데에는 한계가 있다는 점을 느꼈을 것이다. 바로 이 지점에서 DSPy의 차별성이 드러난다. DSPy는 사전에 정의한 메트릭(metric)에 기반해 모듈의 성능을 측정하고, 그 결과를 통해 성능을 튜닝하는 것을 목표로 한다.\n",
    "\n",
    "따라서 LLM 출력의 이상적인 상태가 무엇인지, 그리고 그것을 어떻게 측정할 것인지를 깊이 고민해야 한다. 이는 분류 문제에서는 단순한 정확도(accuracy)가 될 수도 있고, 검색 기반 생성(RAG)과 같은 경우에는 **검색된 컨텍스트에 대한 충실도(faithfulness)** 처럼 더 복잡한 기준이 될 수도 있다.\n",
    "\n",
    "### Example 데이터 타입\n",
    "\n",
    "DSPy에서 평가기(evaluator)와 메트릭이 사용하는 데이터 타입은 Example 객체다. 본질적으로는 딕셔너리(dict)에 가깝지만, DSPy 백엔드가 기대하는 형식에 맞게 데이터를 정리하고 처리해준다.\n",
    "\n",
    "필드는 원하는 대로 자유롭게 정의할 수 있지만, 현재 사용 중인 모듈의 입력·출력 포맷과 반드시 일치하도록 구성해야 한다.\n",
    "\n",
    "학습 데이터셋(training set)은 이러한 Example 객체들의 리스트로 구성된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example({'question': 'What is my name?', 'answer': 'Your name is Adam Lucek'}) (input_keys=None)\n",
      "What is my name?\n",
      "Your name is Adam Lucek\n"
     ]
    }
   ],
   "source": [
    "qa_pair = dspy.Example(question=\"What is my name?\", answer=\"Your name is Adam Lucek\")\n",
    "\n",
    "print(qa_pair)\n",
    "print(qa_pair.question)\n",
    "print(qa_pair.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example({'excerpt': 'I really lova programming!', 'classification': 'Positive', 'confidence': 0.95}) (input_keys=None)\n",
      "I really lova programming!\n",
      "Positive\n",
      "0.95\n"
     ]
    }
   ],
   "source": [
    "classification_pair = dspy.Example(excerpt=\"I really lova programming!\", classification=\"Positive\", confidence=0.95)\n",
    "\n",
    "print(classification_pair)\n",
    "print(classification_pair.excerpt)\n",
    "print(classification_pair.classification)\n",
    "print(classification_pair.confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한 .with_inputs() 메서드를 사용해 입력(input)과 정답(label)을 명시적으로 구분할 수도 있다.\n",
    ".with_inputs()에 지정되지 않은 필드들은 정답(label) 이거나 메타데이터(metadata) 로 간주된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example with input fields only:  Example({'article': 'Placeholder for Article'}) (input_keys={'article'})\n",
      "\n",
      "Example object Non-Input fields only:  Example({'summary': 'Expected Summary'}) (input_keys=None)\n"
     ]
    }
   ],
   "source": [
    "article_summary = dspy.Example(article=\"Placeholder for Article\", summary=\"Expected Summary\").with_inputs(\"article\")\n",
    "\n",
    "input_key_only = article_summary.inputs()\n",
    "non_input_key_only = article_summary.labels()\n",
    "\n",
    "print(\"Example with input fields only: \", article_summary.inputs())\n",
    "print(\"\\nExample object Non-Input fields only: \", article_summary.labels())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "<img src=\"./Media/metrics.png\">\n",
    "\n",
    "이제 데이터 포맷을 이해했으니, **메트릭(metric)** 을 고민해야 한다. 메트릭은 DSPy에서 매우 핵심적인 요소로, 프레임워크는 정의된 메트릭을 기준으로 모듈을 최적화한다.\n",
    "\n",
    "DSPy에서 메트릭은 간결하게 정의된다. 메트릭이란, 데이터에 포함된 Example과 시스템의 출력 결과를 입력으로 받아, 그 출력이 얼마나 좋은지를 수치로 반환하는 함수일 뿐이다. 그렇다면 질문은 이것이다. 당신의 시스템 출력은 어떤 기준에서 ‘좋다’ 혹은 ‘나쁘다’고 판단할 수 있는가?\n",
    "\n",
    "### Simple Metrics\n",
    "\n",
    "<img src=\"./Media/simple_metrics.png\" width=\"300\" >\n",
    "\n",
    "먼저 가장 단순한 형태로 시작해 보자.\n",
    "감성 분류(sentiment classification) 모듈을 대상으로, 정확히 일치하는지(exact match) 여부를 기준으로 검증(validation)을 설정하고 실행한다.\n",
    "\n",
    "#### 모듈 설정 (Setup Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Tweet Sentiment Classification Module\n",
    "from typing import Literal\n",
    "\n",
    "class TwtSentiment(dspy.Signature):\n",
    "    tweet: str = dspy.InputField(desc=\"Candidate tweet for classification\")\n",
    "    sentiment: Literal[\"positive\", \"negative\", \"neutral\"] = dspy.OutputField()\n",
    "\n",
    "twt_sentiment = dspy.ChainOfThought(TwtSentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터셋 포맷 구성 (Format Dataset)\n",
    "\n",
    "[MTEB Tweet Sentiment Extraction](https://huggingface.co/datasets/mteb/tweet_sentiment_extraction) 데이터셋에서 트윗과 감성 레이블(sentiment)로 이루어진 예제들을 가져온다.\n",
    "이 데이터셋이 우리가 검증(validation)에 사용할 기준 데이터가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Formatting Examples\n",
    "examples = []\n",
    "num_examples = 50\n",
    "\n",
    "with open(\"./datasets/train.jsonl\", 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if num_examples and i >= num_examples:\n",
    "            break\n",
    "        data = json.loads(line.strip())\n",
    "        example = dspy.Example(\n",
    "            tweet=data['text'],\n",
    "            sentiment=data['label_text']\n",
    "        ).with_inputs(\"tweet\")\n",
    "        examples.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example({'tweet': 'My Sharpie is running DANGERously low on ink', 'sentiment': 'negative'}) (input_keys={'tweet'})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 메트릭 정의 (Defining Metric)\n",
    "\n",
    "이 메트릭은 Example, Prediction, 그리고 선택적인 trace를 입력으로 받는다 (trace는 이후에 다룰 예정이다).\n",
    "\n",
    "이 경우 메트릭은 LLM이 예측한 감성 값이 정답(ground truth)과 동일한지 여부에 따라 True 또는 False를 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_answer(example, pred, trace=None):\n",
    "    return example.sentiment.lower() == pred.sentiment.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 수동 평가 실행 (Running a Manual Evaluation)\n",
    "\n",
    "각 예제에 포함된 트윗에 대해, 정의해 둔 입력값(트윗)을 사용해 예측을 한 번씩 실행한다.\n",
    "\n",
    "이 예측 결과는 validate_answer 메트릭으로 전달되며, 메트릭은 True 또는 False를 반환한다. 이 결과들은 차례로 scores 리스트에 저장된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for x in examples:\n",
    "    pred = twt_sentiment(**x.inputs())\n",
    "    score = validate_answer(x, pred)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = sum(scores) / len(scores)\n",
    "print(\"Baseline Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 중간 단계 메트릭 (Intermediate Metrics)\n",
    "\n",
    "<img src=\"./Media/inter_metrics.png\" width=\"300\">\n",
    "\n",
    "정답과의 직접 비교 방식도 충분히 유용하지만, **장문 출력(long-form output)** 을 비교·평가하는 데에는\n",
    "LLM-as-a-Judge 방식이 효과적이라는 점도 확인되어 왔다.\n",
    "\n",
    "이제 LLM 기반 메트릭을 구현해 보자.\n",
    "\n",
    "#### 모듈 설정 (Setup Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoT For Summarizing a Dialogue\n",
    "\n",
    "dialog_sum = dspy.ChainOfThought(\"dialogue: str -> summary: str\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 포맷 구성 (Format Dataset)\n",
    "\n",
    "이번 예제에서 사용하는 데이터셋은 DialogSum으로, 여러 대화(dialogue)와 그에 대응하는 요약(summary)으로 구성된 컬렉션이다.\n",
    "\n",
    "이 데이터셋에 포함된 **요약문을 ‘정답(gold standard)’** 으로 삼고, LLM 기반의 **퍼지 메트릭(fuzzy metrics)** 을 사용해 모델 출력을 평가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 20\n",
    "dialogsum_examples = []\n",
    "\n",
    "with open(\"./datasets/dialogsum.train.jsonl\", 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if num_examples and i >= num_examples:\n",
    "            break\n",
    "        data = json.loads(line.strip())\n",
    "        example = dspy.Example(\n",
    "            dialogue=data['dialogue'],\n",
    "            summary=data['summary']\n",
    "        ).with_inputs(\"dialogue\")\n",
    "        dialogsum_examples.append(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 메트릭 시그니처 (Metric Signature)\n",
    "\n",
    "이제 메트릭 내부에서도 모듈을 사용하게 되었으므로, 메트릭 예측(metric prediction)에 적용할 수 있는 동적인 시그니처가 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the signature for autimatic assessments\n",
    "class Assess(dspy.Signature):\n",
    "    \"\"\"Assess the quality of a dialog summary along the specified dimensions.\"\"\"\n",
    "    assessed_text = dspy.InputField()\n",
    "    assessment_question = dspy.InputField()\n",
    "    assessment_answer: bool = dspy.OutputField()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 메트릭 정의 (Metric Definition)\n",
    "\n",
    "이 메트릭에서는 LLM을 평가자로 사용해,\n",
    "생성된 대화 요약이 원본 질문(또는 대화 내용)에 비해 정확한지,\n",
    "그리고 기대 요약(expected summary)에 비해 간결한지를 판단한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dialog_metric(gold, pred, trace=None):\n",
    "    dialogue, gold_summary, generated_summary = gold.dialogue, gold.summary, pred.summary\n",
    "    # Define Assessment Questions\n",
    "    accurate_question = f\"Given this original dialog: '{dialogue}', does the summary accurately represent what was discussed without adding or achnging information?\"\n",
    "    concise_question = f\"\"\"Compare the level of detail in the generated summary with the gold summary:\n",
    "    Gold summary: '{gold_summary}'\n",
    "    Is the generated summary appropriately detailed - neither too sparse nor too verbose compared to the colg summary\"\"\"\n",
    "    # Run Predictions\n",
    "    accurate = dspy.Predict(Assess)(assessed_text=generated_summary, assessment_question=accurate_question)\n",
    "    concise = dspy.Predict(Assess)(assessed_text=generated_summary, assessment_question=concise_question)\n",
    "    # Extract boolean assessment answers\n",
    "    accurate, concise = [m.assessment_answer for m in [accurate, concise]]\n",
    "    # Calculate score - accuracy is required for any points\n",
    "    score = (accurate + concise) if accurate else 0\n",
    "\n",
    "    if trace is not None:\n",
    "        return score >= 2\n",
    "\n",
    "    return score / 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 평가 실행 (Running Evaluation)\n",
    "\n",
    "앞에서 했던 방식과 동일하게, 수동 평가(manual evaluation)를 다시 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_scores = []\n",
    "for x in dialogsum_examples:\n",
    "    pred = dialog_sum(**x.inputs())\n",
    "    score = dialog_metric(x, pred)\n",
    "    intermediate_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = sum(intermediate_scores) / len(intermediate_scores)\n",
    "print(\"Dialog Metric Score: \", final_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DSPy에서 트레이싱을 활용한 고급 메트릭 (Advanced Metrics with Tracing in DSPy)\n",
    "\n",
    "<img src=\"./Media/advan_metrics.png\" width=\"300\">\n",
    "\n",
    "DSPy 문서에서는 메트릭으로 모듈을 사용할 때의 두 가지 핵심 포인트를 강조한다.\n",
    "\n",
    "1. 메트릭 자체가 DSPy 프로그램인 경우, 가장 강력한 반복 개선 방법 중 하나는 메트릭 자체를 컴파일(최적화)하는 것이다.\n",
    "이는 보통 매우 쉽다. 메트릭의 출력은 대개 단순한 값(예: 5점 만점의 점수)이기 때문에, **메트릭을 평가하는 메트릭(metric의 metric)** 을 정의하기 쉽고, 소수의 예제만으로도 최적화가 가능하다.\n",
    "\n",
    "2. 메트릭이 평가(evaluation) 단계에서 사용될 때, DSPy는 프로그램의 내부 단계를 추적하지 않는다.\n",
    "하지만 컴파일(최적화) 단계에서는, DSPy가 **언어 모델 호출을 트레이싱(trace)** 한다. 이 트레이스에는 각 DSPy predictor의 입력과 출력이 포함되며, 이를 활용해 중간 단계를 검증하고 최적화에 활용할 수 있다.\n",
    "\n",
    "두 번째 포인트를 앞선 예제를 통해 좀 더 자세히 살펴보면, 이 메트릭은 두 가지 모드로 동작한다.\n",
    "\n",
    "**표준 평가 모드 (Standard Evaluation, trace=None)**: 요약의 정확성과 간결성을 기준으로 0~1 사이로 정규화된 점수를 반환한다. 이때 **사실적 정확성(factual accuracy)** 은 게이팅 조건으로 작동한다.\n",
    "\n",
    "**컴파일 모드 (Compilation Mode, trace 사용 가능)**: 컴파일 과정에서는 DSPy가 ChainOfThought 모듈(dialog_sum)의 트레이스를 제공한다. 표준 평가에서는 0~1 범위의 점수를 반환하지만, 컴파일 모드에서는 반환 로직을 변경해 **이진 성공 기준(예: score ≥ 2)** 을 반환하도록 한다. 이러한 **이진 신호(success / failure)** 는 각 예제에 대해 명확한 판단 기준을 제공하므로, DSPy가 컴파일 과정에서 훨씬 효과적으로 최적화할 수 있게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dialog_metric(gold, pred, trace=None):\n",
    "    dialogue, gold_summary, generated_summary = gold.dialogue, gold.summary, pred.summary\n",
    "\n",
    "    # LLM-based assessment using Assess signature\n",
    "    accurate = dspy.Predict(Asess)(assessed_text=generated_summary, assessment_question=accurate_question)\n",
    "    concise = dspy.Predict(Assess)(assessed_text=generated_summary, assessment_question=concise_question)\n",
    "\n",
    "    if trace is not None:\n",
    "        # During compilation: Can access and validate CoT reasoning steps\n",
    "        # We're not doing anything with it currently but you can access in this way\n",
    "        reasoning_steps = [output.reasoning for *_, output in trace if hasattr(output, 'reasoning')]\n",
    "        # Return binary success criteria for optimization\n",
    "        return score >= 2  # Requires both accuracy and conciseness\n",
    "\n",
    "    return score / 2.0  # Normalized evaluation score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "트레이스(trace) 기능은 ChainOfThought 구현처럼 복잡한 모듈에서 특히 큰 가치를 가진다.\n",
    "이 기능은 DSPy가 최적화를 수행하는 방식을 바꿔 놓는다. 컴파일 단계에서는 정규화된 점수를 반환하는 대신, 특정 기준(예: score ≥ 2)을 만족하는지 여부에 따른 이진 성공 신호를 제공한다. 이러한 이진 피드백은 각 예제에 대해 명확한 성공/실패 신호를 제공하므로, DSPy가 모델을 훨씬 효과적으로 최적화할 수 있게 해준다.\n",
    "\n",
    "이처럼 **이중 모드 평가 전략(dual-mode evaluation)**은 서로 다른 두 가지 목적을 수행한다.\n",
    "일반 평가 단계에서는 모델 성능을 정밀하게 파악할 수 있도록 세부적인 정규화 점수를 제공하고,\n",
    "컴파일 단계에서는 이진 성공 기준으로 전환해 최적화 과정을 보다 명확하게 유도한다.\n",
    "\n",
    "이 접근법을 통해 우리는 풍부한 평가 메트릭을 유지하면서도, 컴파일 단계에서는 모델 개선에 필요한 명확한 신호를 제공할 수 있다. 나아가, 일반적으로는 드러나지 않는 중간 단계(intermediate step)의 신호들까지 포함해 메트릭을 더욱 정교하게 확장하는 것도 가능하다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최적화 (Optimization)\n",
    "\n",
    "<img src=\"./Media/optimizers.png\">\n",
    "\n",
    "이제 평가에 사용할 모듈과 메트릭이 준비되었으니, 마지막 단계인 **프로그램 최적화(Optimization)** 로 넘어갈 수 있다. 이 과정은 프롬프트를 감으로 수정하고 반복하는 일을 없애고, 측정 가능한 값에 기반해 자동으로 테스트·평가·반복 개선을 수행하게 해준다.\n",
    "\n",
    "DSPy는 프로그램을 최적화하기 위해 여러 가지 방법을 제공한다. 아래 내용은 DSPy 공식 문서에서 정리한 것이다.\n",
    "\n",
    "**자동 Few-Shot 러닝 (Automatic Few-Shot Learning)** 이 계열의 옵티마이저는 시그니처를 확장해, 최적화된 예제들을 자동으로 생성하고 프롬프트에 포함함으로써 few-shot learning을 구현한다.\n",
    "\n",
    "* **LabeledFewShot** : 제공된 라벨된 입력–출력 데이터로부터 few-shot 예제를 구성. 파라미터: k: 프롬프트에 포함할 예제 수, trainset: k개의 예제를 무작위로 선택할 학습 데이터\n",
    "\n",
    "* **BootstrapFewShot** : 교사(teacher) 모듈을 사용해 프로그램의 각 단계에 대한 완전한 데모를 생성(기본값은 현재 프로그램)\n",
    "\n",
    "trainset의 라벨 예제와 함께 사용\n",
    "\n",
    "주요 파라미터:\n",
    "\n",
    "max_labeled_demos: trainset에서 선택할 데모 수\n",
    "\n",
    "max_bootstrapped_demos: 교사가 추가로 생성할 데모 수, 부트스트래핑 과정에서 메트릭을 통과한 데모만 컴파일된 프롬프트에 포함\n",
    "\n",
    "고급 기능: 구조가 호환된다면 다른 DSPy 프로그램을 교사로 사용 가능 (난이도 높은 작업에 유용)\n",
    "\n",
    "* **BootstrapFewShotWithRandomSearch**: BootstrapFewShot을 여러 번 실행하면서 랜덤 서치를 수행\n",
    "\n",
    "그중 가장 성능이 좋은 프로그램을 선택\n",
    "\n",
    "추가 파라미터:\n",
    "\n",
    "num_candidate_programs: 평가할 후보 프로그램 수\n",
    "\n",
    "평가 대상에는 다음이 포함됨: 컴파일되지 않은 원본 프로그램\n",
    "\n",
    "LabeledFewShot 최적화 결과\n",
    "\n",
    "예제가 고정된 BootstrapFewShot 결과\n",
    "\n",
    "예제가 랜덤화된 여러 BootstrapFewShot 결과\n",
    "\n",
    "* **KNNFewShot** : k-최근접 이웃(KNN) 알고리즘을 사용해\n",
    "입력과 가장 유사한 학습 예제를 찾음\n",
    "\n",
    "해당 예제들을 BootstrapFewShot의 trainset으로 사용\n",
    "\n",
    "실전 예시는 별도 노트북 참고\n",
    "\n",
    "**자동 지시문 최적화 (Automatic Instruction Optimization)** 이 계열은 프롬프트에 들어가는 instruction 자체를 최적화한다.\n",
    "MIPROv2의 경우 few-shot 예제까지 함께 최적화할 수 있다.\n",
    "* **COPRO** :\n",
    "\n",
    "각 단계에 대해 새로운 instruction을 생성·개선\n",
    "\n",
    "**좌표 상승법(coordinate ascent, hill-climbing)**으로 최적화\n",
    "\n",
    "파라미터:\n",
    "\n",
    "depth: instruction 개선 반복 횟수\n",
    "\n",
    "* **MIPROv2**:\n",
    "\n",
    "매 단계마다 instruction과 few-shot 예제를 함께 생성\n",
    "\n",
    "데이터와 데모를 인식하는 방식으로 instruction 생성\n",
    "\n",
    "베이지안 최적화를 사용해\n",
    "instruction/데모 조합 공간을 효율적으로 탐색\n",
    "\n",
    "**자동 파인튜닝 (Automatic Finetuning)** 기반 LLM 자체를 파인튜닝하는 방식의 옵티마이저다.\n",
    "\n",
    "* BootstrapFinetune :\n",
    "\n",
    "프롬프트 기반 DSPy 프로그램을 가중치 업데이트로 증류\n",
    "\n",
    "결과물은:\n",
    "\n",
    "동일한 단계 구조를 유지하되\n",
    "\n",
    "각 단계가 프롬프트가 아닌 파인튜닝된 모델로 수행되는 DSPy 프로그램\n",
    "\n",
    "**프로그램 변환 (Program Transformations)**\n",
    "\n",
    "* Ensemble :\n",
    "\n",
    "여러 DSPy 프로그램을 앙상블\n",
    "\n",
    "전체를 사용하거나 일부를 랜덤 샘플링해 하나의 프로그램으로 구성\n",
    "\n",
    "**트윗 데이터 학습 및 테스트 로딩**\n",
    "\n",
    "이번 예제에서는 앞서 사용한 트윗 감성 분류(tweet sentiment classification) 모듈을 최적화한다.\n",
    "분류 문제는 LLM 활용 사례로서 가장 좋은 예는 아니지만,\n",
    "각 옵티마이저가 내부적으로 어떻게 동작하는지 가볍게 이해하기에는 적합하다.\n",
    "\n",
    "이를 통해 이후 더 복잡하고 고급한 DSPy 프로그램에\n",
    "이 최적화 기법들을 효과적으로 적용할 수 있게 된다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Formatting Examples\n",
    "twitter_train = []\n",
    "twitter_test = []\n",
    "train_size = 100 # how many for train\n",
    "test_size = 200  # how many for test\n",
    "\n",
    "with open(\"./datasets/train.jsonl\", 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= (train_size + test_size):\n",
    "            break\n",
    "\n",
    "        data = json.loads(line.strip())\n",
    "        example = dspy.Example(\n",
    "            tweet = data['text'],\n",
    "            sentiment = data['label_text']\n",
    "        ).with_inputs(\"tweet\")\n",
    "\n",
    "        if i < train_size:\n",
    "            twitter_train.append(example)\n",
    "        else:\n",
    "            twitter_test.append(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 후보 프로그램 (Candidate Program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Tweet Sentiment Classification Module\n",
    "from typing import Literal\n",
    "\n",
    "class TwtSentiment(dspy.Signature):\n",
    "    tweet: str = dspy.InputField(desc=\"Candidate tweet for classification\")\n",
    "    sentiment: Literal[\"positive\", \"negative\", \"neutral\"] = dspy.OutputField()\n",
    "\n",
    "base_twt_sentiment = dspy.Predict(TwtSentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_answer(example, pred, trace=None):\n",
    "    return example.sentiment.lower() == pred.sentiment.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_scores = []\n",
    "for x in twitter_test:\n",
    "    pred = base_twt_sentiment(**x.inputs())\n",
    "    score = validate_answer(x, pred)\n",
    "    baseline_scores.append(score)\n",
    "\n",
    "base_accuracy = baseline_scores.count(True) / len(baseline_scores)\n",
    "print(\"Baseline Accuracy: \", base_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 각 프로그램에 적용할 예제 트윗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected Positive Label\n",
    "example_tweet = \"Hi! Waking up, and not lazy at all. You would be proud of me, 8 am here!!! Btw, nice colour, not burnt.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 자동 Few-Shot 학습 (Automatic Few-Shot Learning)\n",
    "\n",
    "<img src=\"./Media/auto_fewshot.png\" width=\"300\">\n",
    "\n",
    "이 옵티마이저들은 추론 시점에 학습 데이터에서 쿼리와 유사한 예제를 찾아 제공하거나,\n",
    "혹은 프로그램 자체로부터 최적화된 예제를 생성해 사용하는 방식에 초점을 둔다.\n",
    "\n",
    "#### LabeledFewShot\n",
    "\n",
    "<img src=\"./Media/label_few_shot.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
